{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo9bRJ1DKZkK"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFhpVw_NLx45",
        "outputId": "7ef096a4-976e-4127-b7dc-dbfd8791e950"
      },
      "outputs": [],
      "source": [
        "import os, torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PE2vJomP3i6"
      },
      "source": [
        "#### Variables to change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "cJYOqyHOP7Yt"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters for training\n",
        "image_resize = 112\n",
        "no_of_conv_layers = 3\n",
        "conv1_out_channel = 40\n",
        "learning_rate = 0.001\n",
        "n_epochs = 200\n",
        "early_stop_patience = 50\n",
        "train_batch_size = 24\n",
        "test_batch_size = 60\n",
        "\n",
        "# Number of runs for training\n",
        "n_runs = 1\n",
        "save_best_model = False\n",
        "\n",
        "# Directories\n",
        "main_dir = os.getcwd()\n",
        "train_dir = f\"{main_dir}/train\"\n",
        "test_dir = f\"{main_dir}/test\"\n",
        "predict_dir = f\"{main_dir}/predict\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77rMU6Pyp1n_"
      },
      "source": [
        "#### Fixed variables (DO NOT CHANGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmNcC0iap7Kc",
        "outputId": "275956a7-7c3f-4a1e-f2d2-9860d201355c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "196\n",
            "160\n"
          ]
        }
      ],
      "source": [
        "# Flatten size (e.g. 28x28 image with 3 conv layers = 3 pooling layers, so divide 28 by 2 by 2 = 7, then flatten size = 7 x 7 x final_conv_out_channel)\n",
        "flattened_size = image_resize\n",
        "for i in range(no_of_conv_layers):\n",
        "  flattened_size //= 2\n",
        "flattened_size = flattened_size * flattened_size\n",
        "\n",
        "# Final convolution layer out channel = 2 ** (Number of layers - 1) * conv1_out_channel\n",
        "final_conv_out_channel = 2 ** (no_of_conv_layers - 1) * conv1_out_channel\n",
        "\n",
        "print(flattened_size)\n",
        "print(final_conv_out_channel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpF5H0fqKd7J"
      },
      "source": [
        "#### Function to load data from directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "Wq42EkW-u-FK"
      },
      "outputs": [],
      "source": [
        "def prepare_data(target_dir):\n",
        "  filepaths = []\n",
        "  labels = []\n",
        "\n",
        "  # Get filepaths of all images in dir\n",
        "  images = os.listdir(target_dir)\n",
        "\n",
        "  # Loop through all images to store each image paths as well as their labels\n",
        "  for image in images:\n",
        "    label = image.split(\"_\")[0]                             # e.g. orange_41.jpg -> [\"orange\", \"41.jpg\"] -> \"orange\"\n",
        "    if label not in [\"orange\", \"apple\", \"mixed\", \"banana\"]:\n",
        "        continue\n",
        "    filepaths.append(f\"{target_dir}/{image}\")               # e.g. /train/orange_41.jpg\n",
        "    labels.append(label)\n",
        "\n",
        "  le = LabelEncoder()\n",
        "  labels = le.fit_transform(labels)\n",
        "\n",
        "  return np.array(filepaths), torch.tensor(labels), le.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQC10c_tPF97"
      },
      "source": [
        "#### CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "aO2qdWokN1vr"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    # Convolution layers\n",
        "    # Start with in_channels=3 because RGB\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=conv1_out_channel, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=conv1_out_channel, out_channels=conv1_out_channel * 2, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(in_channels=conv1_out_channel * 2, out_channels=conv1_out_channel * 4, kernel_size=3, padding=1)\n",
        "\n",
        "    # Average Pooling Layer: downsample by a factor of 2.\n",
        "    self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # Fully Connected Layer 1\n",
        "    self.fc1 = nn.Linear(in_features=flattened_size * final_conv_out_channel, out_features=final_conv_out_channel * 2)\n",
        "\n",
        "    # Fully Connected Layer 2\n",
        "    self.fc2 = nn.Linear(in_features=final_conv_out_channel * 2, out_features=4)\n",
        "\n",
        "    # Activation function\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Apply convolution + ReLU + pooling\n",
        "    x = self.pool(self.relu(self.conv1(x)))     # conv1\n",
        "    x = self.pool(self.relu(self.conv2(x)))     # conv2\n",
        "    x = self.pool(self.relu(self.conv3(x)))     # conv3\n",
        "\n",
        "    # Flatten the feature maps\n",
        "    x = x.view(-1, flattened_size * final_conv_out_channel)\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = self.relu(self.fc1(x))                  # fc1\n",
        "    x = self.dropout(x)                         # dropout layer\n",
        "\n",
        "    # Output layer (out_features = 4)\n",
        "    x = self.fc2(x)                             # fc2\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naNGjkKTPOOk"
      },
      "source": [
        "#### Function to perform image augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "gF8p3nYsPO6H"
      },
      "outputs": [],
      "source": [
        "def load_images(filepaths, purpose):\n",
        "  # Instantiate class to transform image to tensor based on purpose\n",
        "  transform = {\n",
        "    \"train\": transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomPerspective(),\n",
        "            transforms.RandomResizedCrop((image_resize, image_resize), (0.5, 1)),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=(-0.01, 0.01)),\n",
        "            transforms.RandomRotation(180),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), value=0, inplace=False),\n",
        "            v2.GaussianNoise(0, 0.05),\n",
        "        ]\n",
        "    ),\n",
        "    \"test\": transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((image_resize,image_resize)),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "    )\n",
        "  }\n",
        "\n",
        "  image_tensors = []\n",
        "  for path in filepaths:\n",
        "      image = Image.open(path).convert(\"RGB\")\n",
        "      img_tensor = transform[purpose](image)\n",
        "      image_tensors.append(img_tensor)\n",
        "\n",
        "  batch_tensor = torch.stack(image_tensors)\n",
        "  return batch_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgI4dNxbJ6I3"
      },
      "source": [
        "#### Function to test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "x7JEBeWQJ6eV"
      },
      "outputs": [],
      "source": [
        "def test(model, criterion, test_filepaths, test_labels, test_classes):\n",
        "  # Set model to testing mode\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, test_correct = 0, 0\n",
        "  test_total_samples = len(test_filepaths)\n",
        "  no_batch_tested = 0\n",
        "  wrong_preds = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in range(0, test_total_samples, test_batch_size):\n",
        "      # Increase number of batches tested so far in this epoch\n",
        "      no_batch_tested += 1\n",
        "\n",
        "      # Load test images in batch to tensor\n",
        "      batch_filepaths = test_filepaths[i : i + test_batch_size]\n",
        "      batch_inputs = load_images(filepaths=batch_filepaths, purpose=\"test\")\n",
        "      batch_labels = test_labels[i : i + test_batch_size]\n",
        "\n",
        "      # Forward pass: compute predicted outputs\n",
        "      outputs = model(batch_inputs)\n",
        "\n",
        "      # Get probability-distributions\n",
        "      probs = torch.softmax(outputs, dim=1)\n",
        "      _, preds = torch.max(probs, dim=1)\n",
        "\n",
        "      # Save and calculate some stats\n",
        "      test_loss += criterion(outputs, batch_labels).item()\n",
        "      test_correct += torch.sum(preds == batch_labels)  # compare predictions with labels\n",
        "\n",
        "      # Save wrong predictions\n",
        "      for j in range(len(preds)):\n",
        "        if preds[j] != batch_labels[j]:\n",
        "          image_name = batch_filepaths[j].split(\"/\")[-1]\n",
        "          wrong_preds.append([image_name, test_classes[preds[j]].item()])\n",
        "\n",
        "  # Average test loss and test accuracy after epoch\n",
        "  avg_test_loss = test_loss / no_batch_tested\n",
        "  test_acc = test_correct / test_total_samples\n",
        "\n",
        "  return avg_test_loss, test_acc, wrong_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX2kVgQ5QOuK"
      },
      "source": [
        "#### Function to train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "rQ93oKf2QQl7"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, train_filepaths, train_labels, test_filepaths, test_labels, test_classes):\n",
        "\n",
        "  history = {\n",
        "    \"epoch\": [],\n",
        "    \"train_loss\": [],\n",
        "    \"train_acc\": [],\n",
        "    \"test_loss\": [],\n",
        "    \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  best_test_loss = float(\"inf\")\n",
        "  epochs_no_improvement = 0\n",
        "  lr = learning_rate\n",
        "  batch_size = train_batch_size\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    # For tracking and printing our training-progress\n",
        "    train_total_samples = len(train_filepaths)\n",
        "    train_loss, train_correct = 0, 0\n",
        "    no_batch_trained = 0\n",
        "\n",
        "    # Shuffle training data\n",
        "    permutation = torch.randperm(train_total_samples)\n",
        "\n",
        "    for i in range(0, train_total_samples, batch_size):\n",
        "      # Increase number of batches trained so far in this epoch\n",
        "      no_batch_trained += 1\n",
        "\n",
        "      # Load training images in batch to tensor\n",
        "      indices = permutation[i : i + batch_size]\n",
        "      batch_filepaths = train_filepaths[indices]\n",
        "      batch_inputs = load_images(filepaths=batch_filepaths, purpose=\"train\")\n",
        "      batch_labels = train_labels[indices]\n",
        "\n",
        "      # Forward pass: compute predicted outputs\n",
        "      outputs = model(batch_inputs)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = criterion(outputs, batch_labels)\n",
        "  \n",
        "      # Backward pass and optimization step\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Get probability-distributions\n",
        "      probs = torch.softmax(outputs, dim=1)\n",
        "      _, preds = torch.max(probs, dim=1)\n",
        "\n",
        "      # Save and calculate some stats\n",
        "      train_loss += loss.item()\n",
        "      train_correct += torch.sum(preds == batch_labels)  # compare predictions with labels\n",
        "\n",
        "    avg_train_loss = train_loss / no_batch_trained\n",
        "    train_acc = train_correct / train_total_samples\n",
        "\n",
        "    # Learning rate decay\n",
        "    if avg_train_loss < 0.25:\n",
        "      lr = 0.0001\n",
        "    elif avg_train_loss < 0.5:\n",
        "      lr = 0.0005\n",
        "    else:\n",
        "      lr = 0.001\n",
        "\n",
        "    # Evaluate model using test data after training model for this epoch\n",
        "    avg_test_loss, test_acc, wrong_preds = test(model, criterion, test_filepaths, test_labels, test_classes)\n",
        "\n",
        "    # Logging\n",
        "    history[\"epoch\"].append(epoch + 1)\n",
        "    history[\"train_loss\"].append(avg_train_loss)\n",
        "    history[\"train_acc\"].append(train_acc.item())\n",
        "    history[\"test_loss\"].append(avg_test_loss)\n",
        "    history[\"test_acc\"].append(test_acc.item())\n",
        "\n",
        "    # Print stats\n",
        "    print(f\"Epoch {epoch + 1}\\n\",\n",
        "          f\"train loss = {avg_train_loss:.5f}, \",\n",
        "          f\"train accuracy = {train_acc * 100:.2f}%\\n\",\n",
        "          f\"test loss = {avg_test_loss:.5f}, \",\n",
        "          f\"test accuracy = {test_acc * 100:.2f}%\\n\",\n",
        "          f\"wrong test preds = {sorted(wrong_preds)}\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if avg_test_loss < best_test_loss:\n",
        "      best_test_loss = avg_test_loss\n",
        "      epochs_no_improvement = 0\n",
        "\n",
        "      # Save best model (lowest test loss) if set to True\n",
        "      if save_best_model:\n",
        "        torch.save(model.state_dict(), f'{main_dir}/bestmodel.pth')\n",
        "    else:\n",
        "      epochs_no_improvement += 1\n",
        "      if epochs_no_improvement > early_stop_patience:\n",
        "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "        break\n",
        "\n",
        "  return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir8-w0Jymn7n"
      },
      "source": [
        "#### Functions to export to csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "giMwgVMUmrxN"
      },
      "outputs": [],
      "source": [
        "def convert_df(data, parameter_name, parameter_value, n_run):\n",
        "  df = pd.DataFrame(data)\n",
        "  df.insert(0, parameter_name, parameter_value)\n",
        "  df.insert(0, \"run\", n_run + 1)\n",
        "\n",
        "  return df\n",
        "\n",
        "def save_to_csv(dir, parameter_name, df: pd.DataFrame, tracker):\n",
        "  output_dir = f\"{dir}/output/{parameter_name}\"\n",
        "  output_path = f\"{output_dir}/{tracker}.csv\"\n",
        "\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "  df.to_csv(output_path, mode=\"a\", index=False, header=not os.path.exists(output_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYVUzX7aKjwj"
      },
      "source": [
        "#### Checking if train and test folder exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYP5m-iWLHE5",
        "outputId": "68bf6644-cf05-4310-e62c-382e305b6a4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "print(os.path.exists(train_dir))\n",
        "print(os.path.exists(test_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Z2y8SnQCGH"
      },
      "source": [
        "#### Model training logic\n",
        "(Load data > Train > Save training data and params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZYus0OIIvXV",
        "outputId": "32aebda0-268b-457a-9c03-4f1e9af14688"
      },
      "outputs": [],
      "source": [
        "def start_training():\n",
        "  changed_parameter = input(\"Input changed parameter: \").strip()\n",
        "  parameter_value = input(\"Input parameter value: \").strip()\n",
        "\n",
        "  for i in range(n_runs):\n",
        "    # Instantiate training model and define loss function + optimiser\n",
        "    model = CNN()\n",
        "    criterion = nn.CrossEntropyLoss()  # define loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Load training and testing data\n",
        "    # Training data\n",
        "    train_filepaths, train_labels, train_classes = prepare_data(train_dir)\n",
        "\n",
        "    # Testing data\n",
        "    test_filepaths, test_labels, test_classes = prepare_data(test_dir)\n",
        "\n",
        "    # Train model\n",
        "    history = train(model, criterion, optimizer, train_filepaths, train_labels, test_filepaths, test_labels, test_classes)\n",
        "\n",
        "    # Save training loss/accuracy and test accuracy to csv\n",
        "    # For each run, will automatically create directory and every subsequent runs will append into csv\n",
        "    history_csv = convert_df(history, changed_parameter, parameter_value, n_run=i)\n",
        "    save_to_csv(main_dir, changed_parameter, history_csv, \"epoc_loss_acc\")\n",
        "\n",
        "    # Save parameters for current run\n",
        "    params = [[\"image_resize\", image_resize], [\"no_of_conv_layers\", no_of_conv_layers], [\"conv1_out_channel\", conv1_out_channel], [\"learning_rate\", learning_rate], [\"n_epochs\", n_epochs], [\"batch_size\", train_batch_size]]\n",
        "    paramsdf = convert_df(params, changed_parameter, parameter_value, n_run=i)\n",
        "    save_to_csv(main_dir, changed_parameter, paramsdf, \"params\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Predict individual test image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model, test_classes):\n",
        "    # Set model to testing mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            image_to_predict = input(\"Input file name of image to predict: \")\n",
        "            image_to_predict = [f\"{predict_dir}/{image_to_predict}.jpg\"]\n",
        "            batch_inputs = load_images(filepaths=image_to_predict, purpose=\"test\")\n",
        "        except:\n",
        "            print(\"File name entered do not exist, only input name of file. E.g. 'banana_81'\")\n",
        "            return\n",
        "\n",
        "        # Forward pass: compute predicted outputs\n",
        "        outputs = model(batch_inputs)\n",
        "\n",
        "        # Get probability-distributions\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        _, preds = torch.max(probs, dim=1)\n",
        "\n",
        "        print(test_classes[preds[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9HwlzQhKLpu"
      },
      "source": [
        "#### Manually test best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPVf5O3vKNl5",
        "outputId": "d39e7fd4-a443-4909-dc09-65d114d36956"
      },
      "outputs": [],
      "source": [
        "def eval_best_model(purpose):    \n",
        "    # Test best model (Manually)\n",
        "    best_model = CNN()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(best_model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Load saved best model\n",
        "    best_model.load_state_dict(torch.load(f\"{main_dir}/bestmodel.pth\"))\n",
        "\n",
        "    # Testing data\n",
        "    test_filepaths, test_labels, test_classes = prepare_data(test_dir)\n",
        "\n",
        "    if purpose == \"eval\":\n",
        "        avg_test_loss, test_acc, wrong_preds = test(best_model, criterion, test_filepaths, test_labels, test_classes)\n",
        "        print(f\"Final test loss = {avg_test_loss:.5f}, test accuracy = {test_acc * 100:.2f}%, wrong predictions = {wrong_preds}\")\n",
        "    elif purpose == \"predict\":\n",
        "        predict(best_model, test_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Main program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apple\n"
          ]
        }
      ],
      "source": [
        "cmd = input(\"'Train' or 'Eval' or 'Predict' mode? \").strip().lower()\n",
        "\n",
        "if cmd == \"train\":\n",
        "    start_training()\n",
        "elif cmd == \"eval\":\n",
        "    eval_best_model(purpose=cmd)\n",
        "elif cmd == \"predict\":\n",
        "    eval_best_model(purpose=cmd)\n",
        "else:\n",
        "    print(\"Please input 'Train' or 'Eval' or 'Predict' only\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlaenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
